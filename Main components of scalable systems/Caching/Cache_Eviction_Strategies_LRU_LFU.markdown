# استراتژی‌های حذف کش: LRU و LFU

این سند به بررسی مفهوم **حذف کش (Cache Eviction)** و دو استراتژی پرکاربرد آن، یعنی **LRU (Least Recently Used)** و **LFU (Least Frequently Used)**، در طراحی سیستم‌های مقیاس‌پذیر می‌پردازد. هدف این است که با ارائه تعاریف، نحوه عملکرد، موارد استفاده، مزایا و معایب، مقایسه‌ها، مثال‌های واقعی و دیاگرام‌ها، یک راهنمای جامع و آموزشی برای یادگیری طراحی سیستم فراهم شود. این محتوا به زبان فارسی و با فرمت مارک‌داون ارائه شده است تا برای مستندسازی آموزشی و یادگیری اصول طراحی سیستم مناسب باشد.

---

## مقدمه: چرا حذف کش در سیستم‌های با محدودیت حافظه ضروری است؟

**کشینگ** تکنیکی است که داده‌های پراستفاده را در حافظه سریع (مانند RAM) ذخیره می‌کند تا دسترسی به آن‌ها سریع‌تر و کم‌هزینه‌تر باشد. با این حال، حافظه کش معمولاً ظرفیت محدودی دارد و نمی‌تواند تمام داده‌ها را ذخیره کند. در چنین شرایطی، **حذف کش (Cache Eviction)** برای آزادسازی فضا و ذخیره داده‌های جدید ضروری است.

**چرا حذف کش مهم است؟**
- **محدودیت حافظه:** حافظه کش (مانند Redis یا Memcached) ظرفیت محدودی دارد و نمی‌تواند همه داده‌ها را نگه دارد.
- **بهینه‌سازی عملکرد:** حذف داده‌های کم‌استفاده به سیستم اجازه می‌دهد تا داده‌های پراستفاده را در دسترس نگه دارد و نرخ موفقیت کش (Cache Hit Rate) را افزایش دهد.
- **مدیریت منابع:** حذف هوشمند داده‌ها از مصرف بیش‌ازحد حافظه جلوگیری می‌کند و عملکرد سیستم را بهبود می‌بخشد.
- **مقیاس‌پذیری:** استراتژی‌های حذف کش به سیستم‌های مقیاس‌پذیر کمک می‌کنند تا با بارهای کاری سنگین کنار بیایند.

این سند دو استراتژی محبوب حذف کش، یعنی LRU و LFU، را بررسی کرده و راهنمایی برای انتخاب استراتژی مناسب ارائه می‌دهد.

---

## حذف کش چیست؟

### تعریف و زمان وقوع
**حذف کش** فرآیندی است که در آن داده‌های قدیمی یا کم‌استفاده از حافظه کش حذف می‌شوند تا فضا برای داده‌های جدید آزاد شود. این فرآیند زمانی رخ می‌دهد که:
- حافظه کش به ظرفیت کامل خود می‌رسد.
- داده‌های جدید باید به کش اضافه شوند، اما فضای کافی وجود ندارد.
- سیستم نیاز به بهینه‌سازی نرخ موفقیت کش (Cache Hit Rate) دارد.

### چرا حذف کش لازم است؟
- **محدودیت‌های فیزیکی:** حافظه RAM یا SSD ظرفیت محدودی دارد و نمی‌تواند تمام داده‌ها را ذخیره کند.
- **بهینه‌سازی عملکرد:** حذف داده‌های کم‌اهمیت به سیستم اجازه می‌دهد تا داده‌های پراستفاده را سریع‌تر در دسترس قرار دهد.
- **جلوگیری از خرابی:** بدون حذف کش، سیستم ممکن است با خطای کمبود حافظه مواجه شود یا عملکرد آن کاهش یابد.

**مثال:** در یک وب‌سایت تجارت الکترونیک، اگر نتایج جستجوی محصولات پرطرفدار در Redis ذخیره شوند و حافظه پر شود، داده‌های قدیمی یا کم‌استفاده باید حذف شوند تا جا برای نتایج جدید باز شود.

---

## مروری بر استراتژی‌های رایج حذف کش

### LRU (Least Recently Used - کم‌اخیراً استفاده‌شده)
#### نحوه عملکرد
- **ایده اصلی:** داده‌هایی که اخیراً کمترین دسترسی را داشته‌اند (کمتر استفاده شده‌اند) ابتدا حذف می‌شوند.
- **پیاده‌سازی:**
  - داده‌ها در یک ساختار داده مانند لیست دوطرفه (Doubly Linked List) یا صف ذخیره می‌شوند.
  - هر زمان که به یک داده دسترسی می‌شود (Cache Hit)، به ابتدای لیست منتقل می‌شود.
  - وقتی فضا پر می‌شود، داده‌ای که در انتهای لیست قرار دارد (کم‌اخیراً استفاده‌شده) حذف می‌شود.
- **پیاده‌سازی نمونه (شبه‌کد):**
  ```python
  class LRUCache:
      def __init__(self, capacity):
          self.capacity = capacity
          self.cache = {}  # Dictionary for O(1) lookup
          self.list = DoublyLinkedList()  # To track order

      def get(self, key):
          if key in self.cache:
              self.list.move_to_front(key)  # Update recency
              return self.cache[key]
          return None

      def put(self, key, value):
          if key in self.cache:
              self.list.move_to_front(key)
              self.cache[key] = value
          else:
              if len(self.cache) >= self.capacity:
                  lru_key = self.list.remove_tail()  # Remove least recent
                  del self.cache[lru_key]
              self.cache[key] = value
              self.list.add_to_front(key)
  ```

#### موارد استفاده و مزایا
- **موارد استفاده:**
  - برنامه‌هایی که داده‌های اخیراً استفاده‌شده احتمالاً دوباره استفاده می‌شوند (مانند فیدهای خبری در شبکه‌های اجتماعی).
  - کشینگ نتایج پرس‌وجوهای پایگاه داده یا صفحات وب.
- **مزایا:**
  - ساده برای پیاده‌سازی و درک.
  - عملکرد خوب برای الگوهای دسترسی که داده‌های اخیراً استفاده‌شده پراستفاده هستند.
  - نرخ موفقیت کش بالا در سناریوهای با دسترسی زمانی (Temporal Locality).

#### مزایا و معایب
- **مزایا:**
  - مناسب برای داده‌هایی با الگوهای دسترسی اخیر (مانند جلسات کاربر).
  - پیچیدگی زمانی O(1) با استفاده از لیست دوطرفه و جدول هش.
- **معایب:**
  - در سناریوهای با دسترسی‌های پراکنده (مانند اسکن‌های بزرگ) ممکن است داده‌های پراستفاده را به اشتباه حذف کند.
  - به فرکانس دسترسی داده‌ها توجه نمی‌کند، فقط به زمان دسترسی اهمیت می‌دهد.

### LFU (Least Frequently Used - کم‌تکرار استفاده‌شده)
#### نحوه عملکرد
- **ایده اصلی:** داده‌هایی که کمترین تعداد دسترسی را داشته‌اند ابتدا حذف می‌شوند.
- **پیاده‌سازی:**
  - تعداد دفعات دسترسی (Frequency) برای هر داده ثبت می‌شود.
  - داده‌ها در یک ساختار داده مانند Heap یا لیست‌های مرتب‌شده بر اساس فرکانس ذخیره می‌شوند.
  - وقتی فضا پر می‌شود، داده‌ای با کمترین فرکانس دسترسی حذف می‌شود. در صورت تساوی فرکانس، ممکن است از معیارهای اضافی (مانند LRU) استفاده شود.
- **پیاده‌سازی نمونه (شبه‌کد):**
  ```python
  class LFUCache:
      def __init__(self, capacity):
          self.capacity = capacity
          self.cache = {}  # Dictionary for O(1) lookup
          self.freq_map = {}  # Map frequency to list of keys
          self.min_freq = 0  # Track minimum frequency

      def get(self, key):
          if key in self.cache:
              value, freq = self.cache[key]
              self.freq_map[freq].remove(key)
              if not self.freq_map[freq]:
                  del self.freq_map[freq]
                  if self.min_freq == freq:
                      self.min_freq += 1
              self.freq_map.setdefault(freq + 1, []).append(key)
              self.cache[key] = (value, freq + 1)
              return value
          return None

      def put(self, key, value):
          if key in self.cache:
              self.get(key)  # Update frequency
              self.cache[key] = (value, self.cache[key][1])
          else:
              if len(self.cache) >= self.capacity:
                  lfu_key = self.freq_map[self.min_freq].pop(0)
                  del self.cache[lfu_key]
                  if not self.freq_map[self.min_freq]:
                      del self.freq_map[self.min_freq]
              self.cache[key] = (value, 1)
              self.freq_map.setdefault(1, []).append(key)
              self.min_freq = 1
  ```

#### موارد استفاده و سناریوهای ایده‌آل
- **موارد استفاده:**
  - برنامه‌هایی که داده‌های پرتکرار احتمالاً دوباره استفاده می‌شوند (مانند لیدربوردها یا محصولات پرطرفدار).
  - سناریوهایی با الگوهای دسترسی پایدار (Stable Access Patterns).
- **سناریوهای ایده‌آل:**
  - سیستم‌های توصیه (مانند محصولات پرطرفدار در آمازون).
  - کشینگ داده‌هایی که تعداد محدودی آیتم پراستفاده دارند.

#### مزایا و معایب
- **مزایا:**
  - مناسب برای داده‌هایی که تعداد دسترسی‌های بالا اهمیت دارد.
  - نرخ موفقیت کش بالا در سناریوهای با دسترسی‌های مکرر (Frequency Locality).
- **معایب:**
  - پیچیدگی پیاده‌سازی بیشتر نسبت به LRU (نیاز به ردیابی فرکانس).
  - مصرف حافظه بیشتر به دلیل ذخیره فرکانس هر داده.
  - ممکن است داده‌های جدید اما پراستفاده را به دلیل فرکانس پایین حذف کند.

---

## مقایسه: LRU در مقابل LFU

| **معیار**               | **LRU**                                           | **LFU**                                           |
|-------------------------|--------------------------------------------------|--------------------------------------------------|
| **معیار حذف**          | کم‌اخیراً استفاده‌شده (زمان آخرین دسترسی)       | کم‌تکرار استفاده‌شده (تعداد دفعات دسترسی)       |
| **پیچیدگی زمانی**      | O(1) با لیست دوطرفه و جدول هش                   | O(1) یا O(log n) بسته به پیاده‌سازی (Heap)       |
| **مصرف حافظه**         | کمتر (فقط ترتیب دسترسی ذخیره می‌شود)             | بیشتر (نیاز به ذخیره فرکانس و مرتب‌سازی)        |
| **عملکرد**             | خوب برای الگوهای دسترسی اخیر                    | خوب برای الگوهای دسترسی مکرر                    |
| **پیچیدگی پیاده‌سازی** | ساده‌تر                                         | پیچیده‌تر                                        |
| **موارد استفاده**      | جلسات کاربر، فیدهای خبری، صفحات وب            | لیدربوردها، محصولات پرطرفدار، توصیه‌ها         |

---

## چه زمانی از کدام استراتژی استفاده کنیم؟

### استفاده از LRU
- **سناریوها:**
  - وقتی داده‌های اخیراً استفاده‌شده احتمالاً دوباره استفاده می‌شوند (مانند جلسات کاربر در یک وب‌سایت).
  - برنامه‌هایی با الگوهای دسترسی زمانی (Temporal Locality)، مانند فیدهای خبری یا صفحات وب.
  - زمانی که سادگی و مصرف حافظه کم اهمیت است.
- **مثال:** کش کردن نتایج جستجوی کاربران در یک وب‌سایت تجارت الکترونیک که معمولاً به محصولات اخیراً مشاهده‌شده دسترسی دارند.

### استفاده از LFU
- **سناریوها:**
  - وقتی داده‌های پرتکرار احتمالاً دوباره استفاده می‌شوند (مانند محصولات پرطرفدار یا لیدربوردها).
  - برنامه‌هایی با الگوهای دسترسی پایدار که تعداد محدودی آیتم پراستفاده دارند.
  - زمانی که نرخ موفقیت کش (Cache Hit Rate) اهمیت بیشتری نسبت به سادگی دارد.
- **مثال:** کش کردن محصولات پرطرفدار در آمازون که بارها توسط کاربران مختلف مشاهده می‌شوند.

**ملاحظات انتخاب:**
- **الگوی دسترسی داده‌ها:** اگر دسترسی‌ها به‌تازگی وابسته است، LRU بهتر است؛ اگر به فرکانس وابسته است، LFU مناسب‌تر است.
- **منابع سیستم:** LRU حافظه کمتری مصرف می‌کند و پیاده‌سازی ساده‌تری دارد.
- **نیازهای عملکرد:** LFU ممکن است نرخ موفقیت کش بالاتری داشته باشد، اما پیچیدگی بیشتری دارد.

---

## مثال‌های واقعی و ابزارهای پیاده‌سازی

1. **Redis:**
   - **پشتیبانی از LRU:** Redis از استراتژی LRU برای حذف داده‌ها در حالت حافظه محدود استفاده می‌کند (تنظیم `maxmemory-policy` به `volatile-lru` یا `allkeys-lru`).
   - **مثال:** اینستاگرام از Redis با LRU برای کش کردن فیدهای کاربران استفاده می‌کند، که داده‌های اخیراً مشاهده‌شده را در اولویت نگه می‌دارد.
   - **ویژگی‌ها:** Redis از LRU تقریبی (Approximate LRU) استفاده می‌کند که با نمونه‌برداری تصادفی، مصرف حافظه را کاهش می‌دهد.

2. **Memcached:**
   - **پشتیبانی از LRU:** Memcached به‌صورت پیش‌فرض از LRU برای حذف داده‌ها استفاده می‌کند.
   - **مثال:** فیسبوک از Memcached با LRU برای کش کردن پروفایل‌های کاربران و فیدهای خبری استفاده می‌کند، که داده‌های اخیراً دسترسی‌شده را در دسترس نگه می‌دارد.

3. **Varnish (پراکسی معکوس):**
   - Varnish از LRU برای مدیریت کش صفحات وب استفاده می‌کند.
   - **مثال:** وب‌سایت‌های خبری از Varnish با LRU برای کش کردن صفحات پربازدید استفاده می‌کنند.

4. **DynamoDB Accelerator (DAX):**
   - **پشتیبانی از LRU:** DAX از LRU برای کش کردن نتایج پرس‌وجوهای DynamoDB استفاده می‌کند.
   - **مثال:** آمازون از DAX با LRU برای کش کردن داده‌های سبد خرید استفاده می‌کند، که تأخیر را به میکروثانیه کاهش می‌دهد.

---

## خلاصه و نکات کلیدی

- **حذف کش** برای مدیریت حافظه محدود در سیستم‌های کشینگ ضروری است تا داده‌های کم‌استفاده حذف شده و جا برای داده‌های جدید باز شود.
- **LRU (Least Recently Used):**
  - داده‌های کم‌اخیراً استفاده‌شده را حذف می‌کند.
  - مناسب برای برنامه‌هایی با الگوهای دسترسی اخیر (مانند جلسات کاربر).
  - ساده و کم‌مصرف از نظر حافظه.
- **LFU (Least Frequently Used):**
  - داده‌های کم‌تکرار استفاده‌شده را حذف می‌کند.
  - مناسب برای برنامه‌هایی با داده‌های پرتکرار (مانند لیدربوردها).
  - پیچیده‌تر و مصرف حافظه بیشتر.
- **انتخاب استراتژی:** LRU برای سادگی و الگوهای دسترسی اخیر مناسب است، در حالی که LFU برای داده‌های پرتکرار و الگوهای پایدار بهتر عمل می‌کند.
- **ابزارها:** Redis و Memcached از LRU پشتیبانی می‌کنند، و Redis همچنین می‌تواند از LFU با تنظیمات خاص استفاده کند.

### دیاگرام LRU

```
[Cache]
Head (Most Recent) --> [Key1] --> [Key2] --> [Key3] --> Tail (Least Recent)
```
**توضیح:** در LRU، داده‌های جدید یا دسترسی‌شده به سر لیست منتقل می‌شوند، و داده در انتهای لیست (کم‌اخیراً استفاده‌شده) حذف می‌شود.

### دیاگرام LFU

```
[Cache]
Freq=3: [Key1]
Freq=2: [Key2, Key3]
Freq=1: [Key4]
```
**توضیح:** در LFU، داده‌ها بر اساس فرکانس دسترسی مرتب می‌شوند، و داده با کمترین فرکانس (مانند Key4) حذف می‌شود.

---

## منابع پیشنهادی برای مطالعه بیشتر

1. *Designing Data-Intensive Applications* نوشته مارتین کلپمن: کتابی جامع برای یادگیری کشینگ و استراتژی‌های حذف.
2. *The System Design Primer* (منبع متن‌باز در GitHub): راهنمایی برای کشینگ و بهینه‌سازی عملکرد.
3. وبلاگ‌های مهندسی:
   - *Redis Blog*: مقالات در مورد تنظیمات `maxmemory-policy` و LRU/LFU.
   - *Facebook Engineering Blog*: توضیحات در مورد استفاده از Memcached با LRU.
   - *AWS Database Blog*: مقالات در مورد DynamoDB DAX و کشینگ.
4. دوره‌های آنلاین:
   - *Grokking the System Design Interview* در DesignGuru.io
   - *System Design Course* در Educative.io
5. مستندات رسمی:
   - [Redis Memory Optimization](https://redis.io/docs/management/optimization/memory-optimization/)
   - [Memcached Wiki](https://github.com/memcached/memcached/wiki)
   - [Amazon DAX Documentation](https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/DAX.html)

---

این سند مفهوم استراتژی‌های حذف کش LRU و LFU را به‌صورت جامع توضیح داده و برای مستندسازی آموزشی و یادگیری طراحی سیستم مناسب است. در صورت نیاز به توضیحات عمیق‌تر یا مثال‌های بیشتر، لطفاً اطلاع دهید!